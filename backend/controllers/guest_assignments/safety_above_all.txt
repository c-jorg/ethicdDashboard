I’ve been an aerospace engineer for over a decade now, and I’ve always prided myself on the work we do. Developing safety-critical systems for aircraft is no small feat, and every day I’m reminded of the weight of the responsibility we carry. Today, though, is different. I sit at my desk, staring at the reports, reviewing the results from our latest software tests. Everything was supposed to be on track—until this.
In one corner of the flight control software, a rare condition has cropped up—a bug, an anomaly. It’s the kind of thing you might never encounter during a regular flight, and yet, if it does occur, the consequences could be catastrophic. The numbers flash before me, each scenario playing out in my head. It’s unlikely, yes, but not impossible. Fixing it? That would require a significant rework of the code, delaying our project by months, maybe more. The team’s been working day and night to meet this deadline, and any setback could mean losing contracts, penalties, and much worse for the company.
Then there’s the matter of my own investments. I’ve got a decent chunk of stock in the company, and I know what a delay would do. We’ve been talking about this upcoming release for months now, and the market’s ready. Any hint of trouble—especially trouble that impacts safety—and the stock will plummet. I’ll feel it, for sure. But maybe more than that, the whole team will feel it. There are jobs, livelihoods, and reputations on the line.
My boss is confident we can push forward without making too much noise about this bug. “It’s a minor glitch,” they said. “Not worth derailing the whole project. We’ve seen worse and still delivered.” But the pit in my stomach won’t go away. I’ve been here long enough to know that small glitches don’t always stay small. And while I trust the testing process, there’s always the unpredictable element—the one flight where conditions align just right. Or just wrong.
I glance at the clock. The decision needs to be made soon, and I’m not sure how much longer I can sit here wrestling with it. I think about the passengers—those I’ll never meet—who rely on this system. I imagine them boarding the aircraft, smiling at the flight attendants, completely unaware of the systems operating behind the scenes, systems I had a hand in certifying. I imagine what it would be like to see something go wrong, to know that I had a choice, but chose not to act.
And yet, the consequences of stopping everything seem equally daunting. The board won’t be happy, the investors will surely react, and we’ll be back at square one, delaying all the progress we’ve made. There’s a lot on the line, and everyone is expecting a green light.
I sit there, staring at the reports, the weight of the decision growing heavier by the second.

Safety Critical Cases:  
John Sterman and James Quinn,“Boeing’s 737 MAX 8 Disasters.” (2023) MIT Management Sloan School. https://mitsloan.mit.edu/sites/default/files/2023-07/Boeing%27s%20737%20MAX%208%20Disasters.IC__0.pdf 
Richard Witkin, “Engineer’s Warning on DC‐10 Reportedly Never Sent.” (1975) New York Times. https://www.nytimes.com/1975/03/12/archives/engineers-warning-on-dc10-reportedly-never-sent.html
